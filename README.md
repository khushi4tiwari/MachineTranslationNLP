### Machine Translation
Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014).

I found, however, that understanding the model well enough to implement it requires unraveling a series of concepts that build on top of each other. I thought that a bunch of these ideas would be more accessible if expressed visually. A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an imagesâ€¦etc) and outputs another sequence of items. A trained model would work like this:

<video src="images/seq2seq_1.mp4" width="320" height="200" controls preload></video>
